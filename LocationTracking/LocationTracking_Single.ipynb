{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a single video session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The animal's center of mass, in x,y coordinates, is then recorded, as well as the distance in pixels that the animal moves from one frame to the next. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Options for summarizing the data are also provided. \n",
    "\n",
    "The notebook was updated by Davor Virag with some new functionality and usability improvements, which are documented in the notebook. Cells starting with `## CONFIG ##` indicate configurable options to be adjusted by the user, those starting with `## CODE ##` need not be edited, and `## INTERACTIVE ##` produce interactive elements for user input. These cells may begin with `%%output size = 100` or similar, which can be changed to adjust the element size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import pickle\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import panel as pn\n",
    "\n",
    "def create_layout(hv_obj):\n",
    "    hv_pane = pn.pane.HoloViews(hv_obj)\n",
    "    layout = pn.Column()\n",
    "    layout.append(hv_pane)\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Directory, file, and ROI settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to close a figure in the cell immediately after it is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "close_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and Configuration Persistence\n",
    "\n",
    "The notebook now supports saving and loading your configuration settings (crop boundaries, ROIs, reference frame parameters, etc.) which enables three use cases:\n",
    "1. **Process a single video with interactive setup**: Set `load_stored` to `False` (default) to go through the interactive setup steps for a new video.\n",
    "2. **Use saved settings for interactive processing of multiple videos**: Set `load_stored` to `True` to load your saved configuration and then run the interactive setup cells to adjust parameters as needed for each new video. Same as before, but fewer work is needed as you start with your previous settings.\n",
    "3. **Batch process multiple videos without interaction**: Set `load_stored` to `True` and generate a config file that you can apply to multiple videos using the batch processing `.py` scripts. The scripts allow you to generate multiple configs for multiple video groups, which is further explained in the scripts themselves.\n",
    "\n",
    "Specifically, when **`load_stored`** is set to `True`, the notebook will attempt to load a previously saved `video_dict` configuration from disk. The `load_video_dict_storefile` will be loaded from the `dpath` directory. At the end of the notebook, the final `video_dict` of this session will be stored as `save_video_dict_storefile` - if it exists, it will be overwritten.\n",
    "\n",
    "`dpath` is also the location where the videos will be loaded from for processing. Note that if you are using a Windows path with backslashes, place an ‘r’ in front of the directory path to avoid an error (e.g., `r\"C:\\Users\\DeniseCaiLab\\Videos\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "load_stored = False\n",
    "dpath = \"/home/davor/ext/3/OF/\"\n",
    "dpath = Path(dpath)\n",
    "load_video_dict_storefile = dpath/\"video_dict_top_left.pickle\"\n",
    "#save_video_dict_storefile = None\n",
    "save_video_dict_storefile = dpath/\"video_dict_top_left.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if load_stored and load_video_dict_storefile.exists():\n",
    "    with open(load_video_dict_storefile, 'rb') as f:\n",
    "        video_dict = pickle.load(f)\n",
    "else:\n",
    "    video_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file` : The filename of the video, including the file extension.\n",
    "\n",
    "`start_s` : Seconds since the beginning of the video when to begin processing. 0 is the beginning, negative values can be used to specify a time relative to the *end* of the video. For example, `-600` will start processing at 5 minutes before the end of the video, and will process the last 5 minutes of the video (if `end_s` is `None`).\n",
    "\n",
    "`end_s` : Seconds since the beginning of the video when to begin processing. If the user would like to process to the end of the video, this can be set to None.\n",
    "\n",
    "`region_names` : If the user would like to measure the time spent in ROIs, a list containing the names of the ROIs should be provided.  A Python list is defined by a set of square brackets, and each ROI name should be placed in quotations, separated by a comma. If no ROIs are to be defined, this can be set to None (i.e., `‘region_names’ : None`).    \n",
    "*(Note by davor: I haven't tested the updated version without any region names specified - please let me know if you do!)*\n",
    "\n",
    "**Semi-automated region definition can be activated here** for OF and EPM. Simply specify either `\"region_names\": \"OF\"` or `\"region_names\": \"EPM\"` to use this functionality. It is described in more detail later on.\n",
    "\n",
    "`dsmpl` : The amount to down-sample each frame. A value of 1 indicates no down-sampling, while a value of 0.25 indicates that each frame will be down-sampled to ¼ its original size.  Note that if down-sampling is performed, all pixel coordinate output will be in the dimensions of the down-sampled video.\n",
    "\n",
    "`stretch` : Allows the user to alter the aspect ratio of the presented output. This is useful when videos have irregular dimensions and are difficult to see (e.g., an aspect ratio of 1:100). The width/height will be scaled by the factor provided. Note that this only affects the appearance of visualizations and does not modify the video or the interpretation of the output.\n",
    "\n",
    "`angle` : A positive or negative decimal number specifying how many degrees to rotate the video. Can be handy when aligning pre-set OF and EPM ROIs which are strictly horizontal/vertical if the video has a slight rotation to it.\n",
    "\n",
    "***Processing going slow?  Consider downsampling!***  Often times tracking does not not require 1080p or whatever high def resolution videos are sometimes acquired using. Try setting `dsmpl` to something lower than 1 to implement downsampling.\n",
    "\n",
    "***Note:*** Options specified here will override any previously stored configuration when `load_stored` is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "video_dict.update({\n",
    "    'dpath'         : dpath,  # do not change here\n",
    "    'file'          : \"rotated_GX010486_top_left_upd.MP4\",\n",
    "    'start_s'       : -600, \n",
    "    'end_s'         : None,\n",
    "    'region_names'  : ['wall_L','wall_R','wall_T','wall_B', 'corner_UL', 'corner_UR', 'corner_BL', 'corner_BR', 'center'],\n",
    "#   'region_names'  : \"OF\",\n",
    "#   'region_names'  : \"EPM\",\n",
    "    'dsmpl'         : 0.5,\n",
    "    'stretch'       : dict(width=1, height=1),\n",
    "    'bins'          : None,\n",
    "    'bin_duration_s': 60,\n",
    "    'angle'         : None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "video_dict[\"fpath\"] = video_dict[\"dpath\"]/video_dict[\"file\"]\n",
    "cap = cv2.VideoCapture(str(video_dict[\"fpath\"]))\n",
    "\n",
    "video_dict[\"fps\"] = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "video_dict[\"start\"] = int(video_dict[\"start_s\"]*video_dict[\"fps\"])\n",
    "video_dict[\"first_frame\"] = video_dict[\"start\"]\n",
    "video_dict[\"bin_duration\"] = int(video_dict[\"bin_duration_s\"]*video_dict[\"fps\"])\n",
    "video_dict[\"vid_duration\"] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_dict[\"end\"] = int(video_dict[\"end_s\"]*video_dict[\"fps\"]) if video_dict[\"end_s\"] is not None else int(cap.get(cv2.CAP_PROP_FRAME_COUNT))-1\n",
    "if video_dict[\"start_s\"] < 0:\n",
    "    video_dict[\"start\"] += video_dict[\"vid_duration\"]\n",
    "    video_dict[\"start_s\"] += video_dict[\"vid_duration\"]/video_dict[\"fps\"]\n",
    "if video_dict[\"end_s\"] < 0:\n",
    "    video_dict[\"end\"] += video_dict[\"vid_duration\"]\n",
    "    video_dict[\"end_s\"] += video_dict[\"vid_duration\"]/video_dict[\"fps\"]\n",
    "if video_dict[\"start\"] > video_dict[\"end\"]:\n",
    "    raise ValueError('ERROR: The specified start point is after the end point, please check video_dict[\"start_s\"] and video_dict[\"end_s\"]')\n",
    "\n",
    "video_dict[\"fname_stem\"] = Path(video_dict[\"file\"]).stem\n",
    "video_dict[\"output_path\"] = video_dict[\"dpath\"]/video_dict[\"fname_stem\"]\n",
    "video_dict[\"output_path\"].mkdir(exist_ok=True)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating a binned summary (i.e. stats per segment of video), bins can be specified in two ways.\n",
    "\n",
    "**`bins_s`**: Custom bin starts and ends in seconds can be specified using the `bins_s` parameter. The timestamps are specified relative to the entire video, which means the first bin can start at the value of `start_s`, but not before. \n",
    "\n",
    "Negative values can be used, as well as `video_dict[\"start_s\"]` and `video_dict[\"end_s\"]` for the beginning and end of the video respectively, as well as arithmetic. No checks are implemented yet, so take care when specifying the values.\n",
    "\n",
    "To illustrate many ways of specifying bins, an example is given with three bins specified. With the assumption that `video_dict[\"start_s\"] = -600` and that the total video duration is `660 s`, each bin covers 10 seconds with a start at `video_dict[\"start_s\"]` for a total of 30 seconds. The specification, in this case, is equivalent to:\n",
    "\n",
    "```python\n",
    "'bins_s': [\n",
    "    (60, 70),\n",
    "    (70, 80),\n",
    "    (80, 90)\n",
    "]\n",
    "```\n",
    "\n",
    "This parameter takes precedence over `bin_duration_s` (i.e. if `bins_s` is specified, `bin_duration_s` is ignored).\n",
    "\n",
    "**`bin_duration_s`**: If `bins` is set to `None`, `bin_duration_s` is checked - the duration of each bin can be specified to generate the bins automatically.\n",
    "\n",
    "If both parameters are set to `None`, a binned summary won't be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "video_dict.update({\n",
    "#   'bins_s'        : [\n",
    "#       (video_dict[\"start_s\"], video_dict[\"start_s\"]+10),\n",
    "#       (-590, -580),\n",
    "#       (80, 90)\n",
    "#   ],\n",
    "    'bins_s'        : None,\n",
    "    'bin_duration_s': 60\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_dict[\"bins_s\"]:\n",
    "    video_dict[\"bins\"] = {}\n",
    "    for i, b in enumerate(video_dict[\"bins_s\"]):\n",
    "        video_dict[\"bins\"][str(i+1)] = (b[0]*video_dict[\"fps\"], b[1]*video_dict[\"fps\"])\n",
    "elif video_dict[\"bin_duration\"] and not video_dict[\"bins_s\"]:\n",
    "    video_dict[\"bins\"] = {}\n",
    "    for i, bin_start in enumerate(range(video_dict[\"start\"], video_dict[\"vid_duration\"], video_dict[\"bin_duration\"])):\n",
    "        video_dict[\"bins\"][str(i+1)] = (bin_start, bin_start+video_dict[\"bin_duration\"])\n",
    "video_dict[\"last_frame\"] = video_dict[\"vid_duration\"]-video_dict[\"start\"]\n",
    "video_dict[\"full_bin\"] = [(video_dict[\"start\"], video_dict[\"last_frame\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To crop video frame, after running code below, select the box selection tool below the image (square with a plus sign). To start drawing region to be included in the analyis, double click image. Double click again to finalize region. If you decide to change region, it is best to rerun this cell and subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size = 300\n",
    "## INTERACTIVE ##\n",
    "\n",
    "#video_dict[\"crop\"] = None\n",
    "img_crp, crop_stream, video_dict = lt.LoadAndCrop(video_dict, cropmethod='Box')\n",
    "video_dict[\"crop\"] = crop_stream\n",
    "#clear_output()\n",
    "#img_crp, video_dict = lt.LoadAndCrop(video_dict, cropmethod=None)\n",
    "#img_crp\n",
    "layout = create_layout(img_crp)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define reference frame for location tracking\n",
    "\n",
    "For location tracking to work, a view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recommended, being simpler to obtain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by averaging the animal out of the video\n",
    "\n",
    "The following code takes a random sample of frames across the session and creates an average of them by taking the median for each pixel.\n",
    "\n",
    "The frames used can be specified using one of three arguments:\n",
    "- `num_frames`: Specify the number of frames which are chosen at random\n",
    "- `segment`: Specify a tuple, e.g. `(200,1000)`, to take the median out of frames from 200 to 1000.\n",
    "- `frames`: Directly specify the frames to use. For example, NumPy can be used to specify a range `frames = np.arange(100,500,5)` would select every 5th frame in the range 100-500). Can be used as `segment`, where `frames=np.arange(200,1000)` does the same as `segment=(200,1000)`.\n",
    "\n",
    "***Note by davor:*** The algorithm of obtaining frames from the video has been changed. Except in the case of very long videos (presumably, >1 hour), it is faster to go through the entire video frame-by-frame and store the ones we need for median calculation, than to actually skip ahead to the frame we need. This will still be fast if you specify a number of frames within e.g. 5 minutes of the video, since the algorithm will skip ahead, read frame-by-frame from the first to the last specified frame, and stop there. If you have a use case where you need to extract frames across the whole length of a long video and the algorithm is very slow, please let me know via GitHub or e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "\n",
    "#video_dict['reference'], img_ref = lt.Reference(video_dict, num_frames=50) \n",
    "#video_dict['reference'], img_ref = lt.Reference(video_dict, frames=np.arange(100,2000,10))\n",
    "video_dict['reference'], img_ref = lt.Reference(video_dict, segment=(200,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 300\n",
    "## INTERACTIVE ##\n",
    "\n",
    "hv.save(img_ref, video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+\"_reference.png\"), fmt='png')\n",
    "\n",
    "layout = create_layout(img_ref)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - Use a video of an empty box\n",
    "\n",
    "The following code allows the user to specify a different file.  Set `video_dict['altfile']` to the alternative filename.  \n",
    "\n",
    "Defining `frames` is necessary if the alternative video is a different length than the reference video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "%%output size = 100\n",
    "\n",
    "video_dict['altfile'] = 'EmptyBox.avi' \n",
    "\n",
    "video_dict['reference'], img_ref = lt.Reference(video_dict, num_frames=50, altfile=True, frames=[0]) \n",
    "img_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Regions of Interest (ROIs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can specify configuration parameters if you're using the OF or EPM preset. If unclear on how the parameters impact the ROI sizes or placements, just try them out - the ROIs will be visualised a few cells below and you can re-run this entire segment with different parameters until the ROIs are satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Field preset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of OF, it is assumed the arena spans the full area of the video frame. This can be achieved via careful cropping and by using the `video_dict[\"angle\"]` parameter. `wall_fraction` defines how wide the wall region will be, expressed as a fraction of the frame width or height. By default, if `calculate_against` is `None`, horizontal wall-adjacent regions are calculated as a fraction of the frame's height, and vertical ones based on the frame width. This works well for square arenas in approximately square videos.\n",
    "\n",
    "`calculate_against` can be set to `height` or `width`, to calculate the wall-adjacent region width as a fraction of the frame's height or width, respectively.\n",
    "\n",
    "If the video is rectangular, the wall-adjacent ROIs won't be of the same width since they are calculated based on both  - let me know if you need a special preset for other arena shapes.\n",
    "\n",
    "For example, if the frame is 400x450 (WxH) pixels, and `\"wall_fraction\": 0.2`, the animal will be counted as in the centre if its centre of mass is at least 80 px (20% of 400 px) away from the vertical walls, or 90 px away from the horizontal walls. If, for example, `calculate_against` is set to `height`, the value will be 90 px away from any of the walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "OF_preset_config = {\n",
    "    \"wall_fraction\": 0.2, # 20% of frame width/height\n",
    "    \"calculate_against\": None\n",
    "#   \"calculate_against\": \"height\"\n",
    "#   \"calculate_against\": \"width\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elevated Plus Maze preset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EPM, the full ROI area is again based off of the whole frame so cropping and angle adjustment in the previous steps serves to align the areas properly here as well.\n",
    "\n",
    "The ROIs will be a plus sign at the centre of the frame, offset on the horizontal axis by `centre_offset_x` pixels (positive = right, negative = left) and on the vertical axis by `centre_y` pixels (positive = down, negative = up).\n",
    "\n",
    "Its vertical and horizontal arms wide as specified by `vertical_arm_width_frac` and `horizontal_arm_height_frac`, as a fraction of the frame's width and height, respectively. The areas outside of the arms are masked so no changes in these areas are registered or tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "EPM_preset_config = {\n",
    "    \"centre_offset_x\": 11, # 11 pixels to the right\n",
    "    \"centre_offset_y\": -1, # 1 pixel up\n",
    "    \"vertical_arm_width_frac\": 0.085, # 8.5% of frame width\n",
    "    \"horizontal_arm_height_frac\": 0.085 # 8.5% of frame height\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "viewpane_dimensions = {\"x\": abs(video_dict[\"crop\"].data[\"x1\"][0]-video_dict[\"crop\"].data[\"x0\"][0]),\n",
    "                       \"y\": abs(video_dict[\"crop\"].data[\"y1\"][0]-video_dict[\"crop\"].data[\"y0\"][0])}\n",
    "max_x = viewpane_dimensions[\"x\"]\n",
    "max_y = viewpane_dimensions[\"y\"]\n",
    "if video_dict[\"region_names\"] == \"OF\":\n",
    "    roi_size = OF_preset_config[\"wall_fraction\"]\n",
    "    roi_width = roi_size*viewpane_dimensions[\"x\"]\n",
    "    roi_height = roi_size*viewpane_dimensions[\"y\"]\n",
    "    if OF_preset_config[\"calculate_against\"] == \"height\":\n",
    "        roi_width = roi_height\n",
    "    elif OF_preset_config[\"calculate_against\"] == \"width\":\n",
    "        roi_height = roi_width\n",
    "    #['wall_L','wall_R','wall_T','wall_B', 'corner_UL', 'corner_UR', 'corner_BL', 'corner_BR', 'center'],\n",
    "    rois = { \"wall_L\" : {\"xs\": [0, 0, roi_width, roi_width],\n",
    "                        \"ys\": [0, max_y, max_y, 0]},\n",
    "            \"wall_R\" : {\"xs\": [max_x-roi_width, max_x-roi_width, max_x, max_x],\n",
    "                        \"ys\": [0, max_y, max_y, 0]},\n",
    "            \"wall_T\" : {\"xs\": [0, 0, max_x, max_x],\n",
    "                        \"ys\": [0, roi_height, roi_height, 0]},\n",
    "            \"wall_B\" : {\"xs\": [0, 0, max_x, max_x],\n",
    "                        \"ys\": [max_y, max_y-roi_height, max_y-roi_height, max_y]},\n",
    "            \"corner_UL\" : {\"xs\": [0, 0, roi_width, roi_width],\n",
    "                            \"ys\": [0, roi_height, roi_height, 0]},\n",
    "            \"corner_UR\" : {\"xs\": [max_x, max_x-roi_width, max_x-roi_width, max_x],\n",
    "                            \"ys\": [0, 0, roi_height, roi_height]},\n",
    "            \"corner_BL\" : {\"xs\": [0, roi_width, roi_width, 0],\n",
    "                            \"ys\": [max_y, max_y, max_y-roi_height, max_y-roi_height]},\n",
    "            \"corner_BR\" : {\"xs\": [max_x, max_x-roi_width, max_x-roi_width, max_x],\n",
    "                            \"ys\": [max_y, max_y, max_y-roi_height, max_y-roi_height]},\n",
    "            \"center\" : {\"xs\": [roi_width, max_x-roi_width, max_x-roi_width, roi_width],\n",
    "                        \"ys\": [roi_height, roi_height, max_y-roi_height, max_y-roi_height]} }\n",
    "    rois_poly = []\n",
    "    for roi, coords in rois.items():\n",
    "        rois_poly.append({(\"x\", \"y\"):list(zip(coords[\"xs\"], coords[\"ys\"]))})\n",
    "        #rois_poly.append({(\"xs\", \"ys\"):[coords[\"xs\"], coords[\"ys\"]]})\n",
    "    rois_poly = hv.Polygons(data=rois_poly)\n",
    "    #print(rois_poly)\n",
    "    roi_data = {\"xs\": [d[\"xs\"] for d in rois.values()],\n",
    "                \"ys\": [d[\"ys\"] for d in rois.values()]}\n",
    "\n",
    "    video_dict[\"roi_stream\"] = lt.DataStub(roi_data)\n",
    "elif video_dict[\"region_names\"] == \"EPM\":\n",
    "    centre_x = viewpane_dimensions[\"x\"]/2+EPM_preset_config[\"centre_offset_x\"]\n",
    "    centre_y = viewpane_dimensions[\"y\"]/2+EPM_preset_config[\"centre_offset_y\"]\n",
    "    roi_halfwidth = EPM_preset_config[\"vertical_arm_width_frac\"]*viewpane_dimensions[\"x\"]/2\n",
    "    roi_halfheight = EPM_preset_config[\"horizontal_arm_height_frac\"]*viewpane_dimensions[\"y\"]/2\n",
    "    #['open_T', 'open_B', 'closed_L', 'closed_R', 'center']\n",
    "    rois = { \"open_T\" : {\"xs\": [centre_x-roi_halfwidth, centre_x-roi_halfwidth, centre_x+roi_halfwidth, centre_x+roi_halfwidth],\n",
    "                        \"ys\": [0, centre_y-roi_halfheight, centre_y-roi_halfheight, 0]},\n",
    "            \"open_B\" : {\"xs\": [centre_x-roi_halfwidth, centre_x-roi_halfwidth, centre_x+roi_halfwidth, centre_x+roi_halfwidth],\n",
    "                        \"ys\": [max_y, centre_y+roi_halfheight, centre_y+roi_halfheight, max_y]},\n",
    "            \"closed_L\" : {\"xs\": [0, centre_x-roi_halfwidth, centre_x-roi_halfwidth, 0],\n",
    "                        \"ys\": [centre_y-roi_halfheight, centre_y-roi_halfheight, centre_y+roi_halfheight, centre_y+roi_halfheight]},\n",
    "            \"closed_R\" : {\"xs\": [max_x, centre_x+roi_halfwidth, centre_x+roi_halfwidth, max_x],\n",
    "                        \"ys\": [centre_y-roi_halfheight, centre_y-roi_halfheight, centre_y+roi_halfheight, centre_y+roi_halfheight]},\n",
    "            \"center\" : {\"xs\": [centre_x-roi_halfwidth, centre_x-roi_halfwidth, centre_x+roi_halfwidth, centre_x+roi_halfwidth],\n",
    "                        \"ys\": [centre_y-roi_halfheight, centre_y+roi_halfheight, centre_y+roi_halfheight, centre_y-roi_halfheight]}\n",
    "    }\n",
    "    mask = { \"UL\" : {\"xs\": [0, centre_x-roi_halfwidth, centre_x-roi_halfwidth, 0],\n",
    "                    \"ys\": [0, 0, centre_y-roi_halfheight, centre_y-roi_halfheight]},\n",
    "            \"UR\" : {\"xs\": [max_x, centre_x+roi_halfwidth, centre_x+roi_halfwidth, max_x],\n",
    "                    \"ys\": [0, 0, centre_y-roi_halfheight, centre_y-roi_halfheight]},\n",
    "            \"BL\" : {\"xs\": [0, centre_x-roi_halfwidth, centre_x-roi_halfwidth, 0],\n",
    "                    \"ys\": [centre_y+roi_halfheight, centre_y+roi_halfheight, max_y, max_y]},\n",
    "            \"BR\" : {\"xs\": [max_x, centre_x+roi_halfwidth, centre_x+roi_halfwidth, max_x],\n",
    "                    \"ys\": [centre_y+roi_halfheight, centre_y+roi_halfheight, max_y, max_y]}\n",
    "    }\n",
    "    #rois_poly = []\n",
    "    #for roi, coords in rois.items():\n",
    "    #    rois_poly.append({(\"x\", \"y\"):list(zip(coords[\"xs\"], coords[\"ys\"]))})\n",
    "    #    #rois_poly.append({(\"xs\", \"ys\"):[coords[\"xs\"], coords[\"ys\"]]})\n",
    "    #rois_poly = hv.Polygons(data=rois_poly)\n",
    "    #print(rois_poly)\n",
    "\n",
    "    roi_data = {\"xs\": [d[\"xs\"] for d in rois.values()],\n",
    "                \"ys\": [d[\"ys\"] for d in rois.values()]}\n",
    "    video_dict[\"roi_stream\"] = lt.DataStub(roi_data)\n",
    "\n",
    "    mask_data = {\"xs\": [d[\"xs\"] for d in mask.values()],\n",
    "                \"ys\": [d[\"ys\"] for d in mask.values()]}\n",
    "    mask_bool = np.zeros(video_dict[\"f0\"].shape)\n",
    "    for submask in range(len(mask_data[\"xs\"])):\n",
    "        x = np.array(mask_data[\"xs\"][submask]) #x coordinates\n",
    "        y = np.array(mask_data[\"ys\"][submask]) #y coordinates\n",
    "        xy = np.column_stack((x,y)).astype(\"uint64\") #xy coordinate pairs\n",
    "        cv2.fillPoly(mask_bool, pts =[xy], color=1) #fill polygon\n",
    "    mask_bool = mask_bool.astype(\"bool\")\n",
    "    video_dict[\"mask\"] = {\n",
    "        \"stream\": lt.DataStub(mask_data),\n",
    "        \"mask\": mask_bool\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing/checking ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cell below, draw regions of interest on presented image in the order you provided them (in Cell 2).  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 300\n",
    "## INTERACTIVE ##\n",
    "\n",
    "img_roi, video_dict['roi_stream'] = lt.ROI_plot(video_dict)\n",
    "hv.save(img_roi, video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+\"_ROIs.png\"), fmt='png')\n",
    "layout = create_layout(img_roi)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask internal regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following code is used to exclude internal portion/s of the field of view from the analysis. After running cell below, draw regions to be excluded.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell. \n",
    "\n",
    "If using the EPM preset, predefined masks should be shown outside of the maze's plus shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 90\n",
    "## INTERACTIVE ##\n",
    "\n",
    "#del(video_dict[\"mask\"])\n",
    "img_mask, video_dict['mask'] = lt.Mask_select(video_dict)\n",
    "hv.save(img_mask, video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+\"_donottrack.png\"), fmt='png')\n",
    "layout = create_layout(img_mask)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Define scale for distance calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Select two points of known distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cell below, click on any two points and the distance between them, in pixel units, will be presented. Will be used to convert pixel distance to other scale. Note that once drawn, points can be dragged or you can click again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 300\n",
    "## INTERACTIVE ##\n",
    "\n",
    "img_scl, video_dict['scale'] = lt.DistanceTool(video_dict)\n",
    "img_scl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Define real-world distance between points\n",
    "\n",
    "Specify the translation between a distance in pixels to a real world distance. `px_distance` here is calculated as the diagonal between two opposite corners of the video frame as an example, but it can just as well be a number in pixels. I used this for a square OF arena 33x33 cm. Note that scale can be any desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "video_dict[\"scale\"] = {\n",
    "    \"px_distance\": np.sqrt(video_dict[\"reference\"].shape[0]**2+video_dict[\"reference\"].shape[1]**2),\n",
    "    \"true_distance\": 47.376,\n",
    "    \"true_scale\": \"cm\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set location tracking parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "`loc_thresh` : This parameter represents a percentile threshold and can take on values between 0-100.  Each frame is compared to the reference frame.  Then, to remove the influence of small fluctuations, any differences below a given percentile (relative to the maximum difference) are set to 0.  We use a value of 99.5 with good success.\n",
    "\n",
    "`use_window` : This parameter is incredibly helpful if objects other than the animal temporarily enter the field of view during tracking (such as an experimenter’s hand manually delivering a stimulus or reward).  When use_window is set to True, a square window with the animal's position on the prior frame at its center is given more weight when searching for the animal’s location (because an animal presumably can't move far from one frame to the next).  In this way, the influence of objects entering the field of view can be avoided.  If use_window is set to True, the user should consider window_size and window_weight.\n",
    "\n",
    "`window_size` : This parameter only impacts tracking when use_window is set to True.  This defines the size of the square window surrounding the animal that will be more heavily weighted in pixel units.  We typically set this to 2-3 times the animal’s size (if an animal is 100 pixels at its longest, we will set window_size to 200).  Note that to determine the size of the animal in pixels, the user can reference any image of the arena presented in ezTrack, which has the pixel coordinate scale on its axes.\n",
    "\n",
    "`window_weight` : This parameter only impacts tracking when use_window is set to True.  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight. Notably, setting a high value that is still not equal to 1 (e.g., 0.9) should allow ezTrack to more rapidly find the animal if, by chance, it moves out of the window.  \n",
    "\n",
    "`method` : This parameter determines the luminosity of the object ezTrack will search for relative to the background and accepts values of 'abs', 'light', and 'dark'. Option 'abs' does not take into consideration whether the animal is lighter or darker than the background and will therefore track the animal across a wide range of backgrounds. 'light' assumes the animal is lighter than the background, and 'dark' assumes the animal is darker than the background. Option 'abs' generally works well, but there are situations in which you may wish to use the others.  For example, if a tether is being used that is opposite in color to the animal (a white wire and a black mouse), the ‘abs’ method is much more likely to be biased by the wire, whereas option ‘dark’ will look for the darker mouse.\n",
    "\n",
    "`rmv_wire` : When rmv_wire is set to True, an algorithm is used to attempt to remove wires from the field of view.  If rmv_wire is set to True, the user should consider wire_krn.\n",
    "\n",
    "`wire_krn` : This parameter only impacts tracking when rmv_wire is set to True. This value should be set between the width of the wire and the width of the animal, in pixel units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "tracking_params = {\n",
    "    'loc_thresh'    : 98.0, \n",
    "    'use_window'    : True, \n",
    "    'window_size'   : 150, \n",
    "    'window_weight' : .9, \n",
    "    'method'        : 'abs',\n",
    "    'rmv_wire'      : True, \n",
    "    'wire_krn'      : 5,\n",
    "    'progress_bar'  : True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Display examples of location tracking to confirm threshold\n",
    "In order to confirm threshold is working, a subset of images is analyzed and displayed using the selected `tracking_params`. The original image is displayed on the left and the difference values to the right. The center of mass (COM) is pinpointed on images. Notably, because individual frames are used, window settings are not applicable here. Because of this, actual tracking in video is likely to be better.\n",
    "\n",
    "The user can change the number examples below as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 250\n",
    "## INTERACTIVE ##\n",
    "\n",
    "img_exmpls = lt.LocationThresh_View(video_dict, tracking_params, examples=16)\n",
    "hv.save(img_exmpls, video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+\"_CheckTracking.png\"), fmt='png')\n",
    "layout = create_layout(img_exmpls.cols(4))\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `video_dict` configuration to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the current `video_dict` configuration to disk in two locations:\n",
    "1. In the video's output directory (`video_dict[\"output_path\"]/last_video_dict.pickle`), so each video has a reference of the `video_dict` config used to process it,\n",
    "2. At the main data path (`save_video_dict_storefile`) for reuse across sessions.\n",
    "\n",
    "This enables:\n",
    "- Resuming work on the same video later without reconfiguring crop, ROIs, and reference frame\n",
    "- Applying the saved configuration to new videos by setting `load_stored = True` at the beginning of the notebook\n",
    "- Using the configuration file for batch processing with the `.py` scripts\n",
    "\n",
    "The saved configuration includes all setup parameters: crop boundaries, ROIs, reference frame, tracking parameters, and scaling information.\n",
    "\n",
    "Tracking parameters are not stored - if you need this, let me know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if video_dict[\"region_names\"] == \"OF\":\n",
    "    video_dict[\"OF_preset_config\"] = OF_preset_config.copy()\n",
    "elif video_dict[\"region_names\"] == \"EPM\":\n",
    "    video_dict[\"EPM_preset_config\"] = EPM_preset_config.copy()\n",
    "\n",
    "with open(video_dict[\"output_path\"]/\"video_dict.pickle\", 'wb') as f:\n",
    "    pickle.dump(lt.copy_video_dict(video_dict), f)\n",
    "if save_video_dict_storefile:\n",
    "    with open(save_video_dict_storefile, 'wb') as f:\n",
    "        pickle.dump(lt.copy_video_dict(video_dict), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track location and save results to a .csv file\n",
    "For each frame the location of the animal's center of mass is recorded in x/y coordinates.  If ROIs are supplied, for each frame it is determined whether the animal is in each of the ROIs.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist, and whether the animal is in each ROI specified (True/False).  Data is saved to a .csv in the same folder as the video.  First 5 rows of data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "location = lt.TrackLocation(video_dict, tracking_params)   \n",
    "#if __name__ == \"__main__\":\n",
    "#    location = lt.TrackLocation_parallel(video_dict, tracking_params)\n",
    "location.to_csv(video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+'_LocationOutput.csv'), index=False)\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Display animal distance/location across session\n",
    "Below, the animals distance and location across the video is plotted.  Smooth traces are expected in the case where the animal is tracked consistently across the session.  Under heatmap, sigma controls 'binning' of location. When `sigma = None` default value is provided; but sigma can also be set to any value. `width` and `height` control the size of the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "sigma = None\n",
    "width = 150\n",
    "height = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 400\n",
    "## INTERACTIVE ##\n",
    "\n",
    "plt_dist = hv.Curve((location['Frame'],location['Distance_px']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"Distance Across Session\",toolbar=\"below\")\n",
    "plt_trks = lt.showtrace(video_dict, location, color=\"red\", alpha=.05, size=2)\n",
    "plt_hmap = lt.Heatmap(video_dict, location, sigma=sigma)\n",
    "trace_img = (plt_trks + plt_hmap + plt_dist).cols(1)\n",
    "hv.save(trace_img, video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+\"_TraceAndHeatmap.png\"), fmt='png')\n",
    "layout = create_layout(trace_img)\n",
    "layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if close_plots:\n",
    "    layout[-1] = pn.pane.Markdown(\"✅ Region captured and cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (binned) summary report and save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates binned and/or overall summary statistics for the session. The bins are defined in `video_dict` configuration at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "if video_dict[\"bins\"]:\n",
    "    summary_binned = lt.Summarize_Location(location, video_dict, bin_dict=video_dict[\"bins\"])\n",
    "    summary_binned.to_csv(video_dict[\"output_path\"]/str(video_dict[\"fname_stem\"]+'_SummaryStats_binned.csv'), index=False)\n",
    "summary = lt.Summarize_Location(location, video_dict, bin_dict=video_dict[\"full_bin\"])\n",
    "summary_full_filename = video_dict[\"dpath\"]/str(video_dict[\"dpath\"].stem+'_SummaryStats.csv')\n",
    "if not summary_full_filename.exists():\n",
    "    summary_full = summary\n",
    "else:\n",
    "    summary_full = pd.read_csv(summary_full_filename)\n",
    "    if summary_full.loc[summary_full[\"File\"] == video_dict[\"file\"]].empty:\n",
    "        summary_full = pd.concat([summary_full, summary])\n",
    "    else:\n",
    "        summary_full = summary_full.set_index(\"File\")\n",
    "        summary_full.update(summary.set_index(\"File\"))\n",
    "        summary_full = summary_full.reset_index()\n",
    "summary_full.to_csv(summary_full_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) View video of tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that tracking must be done before this (Section [Track location](#track-location)). \n",
    "\n",
    "`start` : The frame video playback is to be started on. Note that this is relative to the start of tracking, where 0 is the first tracked frame.\n",
    "\n",
    "`stop` : The frame video playback is to end on.  Note that this is relative to the start of tracking, where 0 is the first tracked frame.\n",
    "\n",
    "`fps` : The speed of video playback.  Must be an integer.  Video playback may also be slower depending upon computer speed. \n",
    "\n",
    "`resize` : If the user wants the output to be larger or smaller, or they want the aspect ratio to be different, resize can be supplied as in the following example:\n",
    "\t`‘resize’ : (100,200)`\n",
    "Here, the first number corresponds to the adjusted width of the frame, whereas the second number corresponds to the adjusted height.  Both numbers reflect pixel units and should be integers. Set resize equal to None if no resizing is to be done.\n",
    "\n",
    "`save_video` : To save the video clip, set to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG ##\n",
    "display_dict = {\n",
    "    'start'      : 0, \n",
    "    'stop'       : video_dict[\"last_frame\"]-video_dict[\"start\"], \n",
    "    'fps'        : video_dict[\"fps\"],\n",
    "    'resize'     : None,\n",
    "    'file'       : video_dict[\"fname_stem\"]+\"_tracked.mkv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE ##\n",
    "lt.SaveVideo(video_dict,display_dict,location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## CODE - UNUSED ##\n",
    "# Experimental attempt to speed up video saving, though largely unsuccessful\n",
    "lt.SaveVideo_fast(video_dict,display_dict,location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezTrack",
   "language": "python",
   "name": "eztrack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
